{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = pd.read_csv(\"../../../../Data_thesis/Full_Datasets/Crowdedness.csv\")\n",
    "gvb_df = pd.read_csv(\"../../../../Data_thesis/Full_Datasets/GVBData.csv\")\n",
    "event_df = pd.read_csv(\"../../../../Data_thesis/Full_Datasets/Events.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strToTimestamp(df, format):\n",
    "    \"\"\"\n",
    "    This function converts a pandas.df column to Timestamp object\n",
    "\n",
    "    Parameters:\n",
    "    - df (df[col]): Needs to be converted to pd.Timestamp\n",
    "    - format (str): format of the date as it's given\n",
    "\n",
    "    Returns: DF[col] with al dates as pd.Timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.to_datetime(df, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startEndDate(df1, df2):\n",
    "    \"\"\"\n",
    "    This function returns the min date of a given df column and the max date of a given df column\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (df[col]): From which the min date has to be returned\n",
    "    - df2 (df[col]) (optional): From which the max date has to be returned\n",
    "        - Optional: If this parameter is not given, the value of df1 will be used\n",
    "        - Useful if max and min date are not in the same df[col]\n",
    "    \n",
    "    Returns: Min and Max date if given column(s)\n",
    "    \"\"\"\n",
    "\n",
    "    #if one dataframe is defined \n",
    "    if df2 is None:\n",
    "        return df1.min(), df1.max()\n",
    "\n",
    "    #If two dataframes are defined\n",
    "    else: \n",
    "        return df1.min(), df2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(sensor_df, gvb_df, event_df):\n",
    "    \"\"\"\n",
    "    This function converts the date from str to pd.Timestamp object \n",
    "    \n",
    "    Parameters:\n",
    "    - sensor_df (df): sensor data\n",
    "    - gvb_df (df): gvb data\n",
    "    - event_df (df): event data\n",
    "\n",
    "    Returns: Returns all Df's with pd.Timestamp objects\n",
    "    \"\"\"\n",
    "\n",
    "    #Variables\n",
    "    \n",
    "    #Format Datetime\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "\n",
    "    #List all df's\n",
    "    df_list = [sensor_df, gvb_df, event_df]\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Loop over all given DF's and transform str to timestamp\n",
    "    for df in df_list:\n",
    "        df[\"Date\"] = strToTimestamp(df[\"Date\"], date_format)\n",
    "\n",
    "    return sensor_df, gvb_df, event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeStartEndDate(sensor_df, gvb_df, event_df):\n",
    "    \"\"\"\n",
    "    This function selects rows of df's based on generated start and end dates\n",
    "    \n",
    "    Parameters:\n",
    "    - sensor_df (df): sensor data\n",
    "    - gvb_df (df): gvb data\n",
    "    - event_df (df): event data\n",
    "\n",
    "    Returns: Returns all DF's within given start and end dates\n",
    "    \"\"\"\n",
    "\n",
    "    #Variables\n",
    "\n",
    "    #Select start and end date\n",
    "    start_date, end_date = startEndDate(sensor_df[\"Date\"], gvb_df[\"Date\"])\n",
    "\n",
    "    #List all df's\n",
    "    df_list = [sensor_df, gvb_df, event_df]\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Loop over all given DF's and select rows based on start and end date\n",
    "    for df in df_list:\n",
    "        df = df[(df[\"Date\"] >= start_date) & (\n",
    "            df[\"Date\"] <= end_date)].reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "    return sensor_df, gvb_df, event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateWeights(stations, df, station_scaler_filename):\n",
    "    \"\"\"\n",
    "    This function returns a dict with scaled rbk kernels, representing the distance between each station and sensor. \n",
    "\n",
    "    Parameters:\n",
    "    - stations (list): all relevant stations\n",
    "    - df (df): where the latitudes and longitudes of each station and sensor are stored\n",
    "    - station_scaler_filename (str): where the scalar for station weights should be stored\n",
    "\n",
    "    Returns: Dict with all scaled weights per sensor, per station\n",
    "    \"\"\"\n",
    "\n",
    "    #Variables\n",
    "\n",
    "    #List all sensors present in full dataset\n",
    "    sensors = df[\"Sensor\"].unique()\n",
    "\n",
    "    #List where the rbf kernels of all stations, in relation to the sensor, will be saved\n",
    "    weights = []\n",
    "\n",
    "    #Scaler for station kernel data\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    #Dict for rbf weight positions in the weights list\n",
    "    weights_dict = {}\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Loop over all the sensors\n",
    "    for sensor in sensors:\n",
    "\n",
    "        #Make an array with the latitude and longitude of the sensor\n",
    "        y = np.array([df[df[\"Sensor\"] == sensor].reset_index()[\"SensorLatitude\"][0],\n",
    "                      df[df[\"Sensor\"] == sensor].reset_index()[\"SensorLongitude\"][0]]).reshape(1, -1)\n",
    "\n",
    "        #Dict where the rbf kernels of all stations, in relation to the sensor, will be saved\n",
    "        stations_dict = {}\n",
    "\n",
    "        #Loop over all stations\n",
    "        for station in stations:\n",
    "\n",
    "            #Make an array with the latitude and longitude of the station\n",
    "            x = np.array([df[station + \" Lat\"][0],\n",
    "                          df[station + \" Lon\"][0]]).reshape(1, -1)\n",
    "\n",
    "            #Save the resulting weight of the RBF kernel between the y(sensor) and x(station) coordinates\n",
    "            weights.append(rbf_kernel(x, y)[0, 0])\n",
    "\n",
    "            #Save the position of the weight in the list\n",
    "            stations_dict[station] = len(weights) - 1\n",
    "\n",
    "        #Save all te rbf kernel weights positions of the list\n",
    "        weights_dict[sensor] = stations_dict\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Convert list to np array and reshape the array\n",
    "    weights = np.asarray(weights)\n",
    "    weights = weights.reshape(-1, 1)\n",
    "\n",
    "    #Scale the weights and save the scaler for later use\n",
    "    weights = scaler.fit_transform(weights)\n",
    "    pickle.dump(scaler, open(station_scaler_filename, 'wb'))\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Loop over weights dict and replace the rbf weights positions with the actual weights\n",
    "    for k, v in weights_dict.items():\n",
    "        for station in stations:\n",
    "            v[station] = weights[v[station]]\n",
    "\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructFullDF(sensor_df, gvb_df, event_df, stations, station_scaler_filename):\n",
    "    \"\"\"\n",
    "    This function combines all the previously constructed DF's and merges them into one. In addition, time is transformed into a cyclic continuous feature.\n",
    "\n",
    "    Parameters:\n",
    "    - sensor_df (df): sensor data\n",
    "    - gvb_df (df): gvb data\n",
    "    - event_df (df): event data\n",
    "    - stations (list): all relevant stations\n",
    "    - station_scaler_filename (str): where the scalar for station weights should be stored\n",
    "\n",
    "    Returns: Full GVB that contains all relevant data\n",
    "    \"\"\"\n",
    "\n",
    "    #Combine DF's\n",
    "    gvb_sensor_df = pd.merge(gvb_df, sensor_df, on=[\n",
    "                            \"Date\", \"Hour\", \"weekday\"], how=\"outer\")\n",
    "    full_df = pd.merge(gvb_sensor_df, event_df, on=[\"Date\"], how=\"outer\")\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Sort keys on date\n",
    "    full_df = full_df.sort_values(\n",
    "        by=[\"Date\"]).reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "    #Fill NaN values with 0.0\n",
    "    full_df = full_df.fillna(0.0)\n",
    "\n",
    "    #Add columns for the cos and sin of month, day and year\n",
    "    full_df = full_df.assign(Year=0, month_sin=0, month_cos=0,\n",
    "                             day_sin=0, day_cos=0, hour_sin=0, hour_cos=0)\n",
    "\n",
    "    for station in stations:\n",
    "        full_df[station + \" score\"] = 0\n",
    "        full_df[station + \" weight\"] = 0\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Construct dict with station weigths\n",
    "    station_weights = calculateWeights(stations, full_df, station_scaler_filename)\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Transform DF to Dict\n",
    "    time_dict = full_df.to_dict(\"index\")\n",
    "\n",
    "    #Transform Date to seperate year, month, day and hour. And transform month, day, hour to cos/sin to make it circular\n",
    "    for k, v in time_dict.items():\n",
    "        v[\"Year\"] = v[\"Date\"].year\n",
    "\n",
    "        v[\"month_sin\"] = np.sin(2 * np.pi * v[\"Date\"].month / 12)\n",
    "        v[\"month_cos\"] = np.cos(2 * np.pi * v[\"Date\"].month / 12)\n",
    "\n",
    "        v[\"day_sin\"] = np.sin(2 * np.pi * v[\"Date\"].day / 365)\n",
    "        v[\"day_cos\"] = np.cos(2 * np.pi * v[\"Date\"].day / 365)\n",
    "\n",
    "        v[\"hour_sin\"] = np.sin(2 * np.pi * v[\"Hour\"] / 2400)\n",
    "        v[\"hour_cos\"] = np.cos(2 * np.pi * v[\"Hour\"] / 2400)\n",
    "\n",
    "        #Loop over all stations\n",
    "        for station in stations:\n",
    "\n",
    "            #Add a station score, which is the weight multiplied with total passengers\n",
    "            v[station + \" score\"] = float(station_weights[v[\"Sensor\"]][station] * (\n",
    "                v[station + \" Arrivals\"] + v[station + \" Departures\"]))\n",
    "\n",
    "            #Add station weight \n",
    "            v[station +\n",
    "                \" weight\"] = float(station_weights[v[\"Sensor\"]][station])\n",
    "\n",
    "    #Transform dict back to DF\n",
    "    full_df = pd.DataFrame.from_dict(\n",
    "        time_dict, orient=\"index\").reset_index().drop(columns=\"index\")\n",
    "\n",
    "    #################################################################################\n",
    "\n",
    "    #Drop nonrelevant columns\n",
    "    for station in stations:\n",
    "        full_df.drop(columns={station + \" Arrivals\",\n",
    "                              station + \" Departures\"}, inplace=True)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct needed DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df, gvb_df, event_df = importData(sensor_df, gvb_df, event_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Start and End Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df, gvb_df, event_df = changeStartEndDate(sensor_df, gvb_df, event_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Full DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [\"Nieuwmarkt\", \"Nieuwezijds Kolk\", \"Dam\", \"Spui\", \"Centraal Station\"]\n",
    "\n",
    "full_df = constructFullDF(\n",
    "        sensor_df, gvb_df, event_df, stations, station_scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Hour', 'Nieuwmarkt Lat', 'Nieuwmarkt Lon',\n",
       "       'Nieuwezijds Kolk Lat', 'Nieuwezijds Kolk Lon', 'Dam Lat', 'Dam Lon',\n",
       "       'Spui Lat', 'Spui Lon', 'Centraal Station Lat', 'Centraal Station Lon',\n",
       "       'weekday', 'is_weekend', 'Sensor', 'SensorLongitude', 'SensorLatitude',\n",
       "       'CrowdednessCount', 'LonScaled', 'LatScaled', 'is_event', 'Year',\n",
       "       'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos',\n",
       "       'Nieuwmarkt score', 'Nieuwmarkt weight', 'Nieuwezijds Kolk score',\n",
       "       'Nieuwezijds Kolk weight', 'Dam score', 'Dam weight', 'Spui score',\n",
       "       'Spui weight', 'Centraal Station score', 'Centraal Station weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Nieuwmarkt Lat</th>\n",
       "      <th>Nieuwmarkt Lon</th>\n",
       "      <th>Nieuwezijds Kolk Lat</th>\n",
       "      <th>Nieuwezijds Kolk Lon</th>\n",
       "      <th>Dam Lat</th>\n",
       "      <th>Dam Lon</th>\n",
       "      <th>Spui Lat</th>\n",
       "      <th>Spui Lon</th>\n",
       "      <th>...</th>\n",
       "      <th>Nieuwmarkt score</th>\n",
       "      <th>Nieuwmarkt weight</th>\n",
       "      <th>Nieuwezijds Kolk score</th>\n",
       "      <th>Nieuwezijds Kolk weight</th>\n",
       "      <th>Dam score</th>\n",
       "      <th>Dam weight</th>\n",
       "      <th>Spui score</th>\n",
       "      <th>Spui weight</th>\n",
       "      <th>Centraal Station score</th>\n",
       "      <th>Centraal Station weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>100</td>\n",
       "      <td>52.371942</td>\n",
       "      <td>4.901239</td>\n",
       "      <td>52.376288</td>\n",
       "      <td>4.893731</td>\n",
       "      <td>52.373127</td>\n",
       "      <td>4.89008</td>\n",
       "      <td>52.369097</td>\n",
       "      <td>4.889259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587521</td>\n",
       "      <td>-7.303229</td>\n",
       "      <td>-0.070905</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.681885</td>\n",
       "      <td>163.979433</td>\n",
       "      <td>0.346680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>52.371942</td>\n",
       "      <td>4.901239</td>\n",
       "      <td>52.376288</td>\n",
       "      <td>4.893731</td>\n",
       "      <td>52.373127</td>\n",
       "      <td>4.89008</td>\n",
       "      <td>52.369097</td>\n",
       "      <td>4.889259</td>\n",
       "      <td>...</td>\n",
       "      <td>352.242352</td>\n",
       "      <td>1.015108</td>\n",
       "      <td>83.818349</td>\n",
       "      <td>0.421198</td>\n",
       "      <td>-448.251607</td>\n",
       "      <td>-0.353790</td>\n",
       "      <td>-130.349894</td>\n",
       "      <td>-0.972760</td>\n",
       "      <td>1668.972876</td>\n",
       "      <td>0.432376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>52.371942</td>\n",
       "      <td>4.901239</td>\n",
       "      <td>52.376288</td>\n",
       "      <td>4.893731</td>\n",
       "      <td>52.373127</td>\n",
       "      <td>4.89008</td>\n",
       "      <td>52.369097</td>\n",
       "      <td>4.889259</td>\n",
       "      <td>...</td>\n",
       "      <td>384.307874</td>\n",
       "      <td>1.107515</td>\n",
       "      <td>8.440208</td>\n",
       "      <td>0.042413</td>\n",
       "      <td>-1899.118956</td>\n",
       "      <td>-1.498910</td>\n",
       "      <td>-352.192566</td>\n",
       "      <td>-2.628303</td>\n",
       "      <td>3994.026557</td>\n",
       "      <td>1.034722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2200</td>\n",
       "      <td>52.371942</td>\n",
       "      <td>4.901239</td>\n",
       "      <td>52.376288</td>\n",
       "      <td>4.893731</td>\n",
       "      <td>52.373127</td>\n",
       "      <td>4.89008</td>\n",
       "      <td>52.369097</td>\n",
       "      <td>4.889259</td>\n",
       "      <td>...</td>\n",
       "      <td>342.287656</td>\n",
       "      <td>1.049962</td>\n",
       "      <td>42.314730</td>\n",
       "      <td>0.253382</td>\n",
       "      <td>-1231.740804</td>\n",
       "      <td>-1.207589</td>\n",
       "      <td>-279.710090</td>\n",
       "      <td>-2.350505</td>\n",
       "      <td>3446.831547</td>\n",
       "      <td>1.043862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2200</td>\n",
       "      <td>52.371942</td>\n",
       "      <td>4.901239</td>\n",
       "      <td>52.376288</td>\n",
       "      <td>4.893731</td>\n",
       "      <td>52.373127</td>\n",
       "      <td>4.89008</td>\n",
       "      <td>52.369097</td>\n",
       "      <td>4.889259</td>\n",
       "      <td>...</td>\n",
       "      <td>343.524310</td>\n",
       "      <td>1.053756</td>\n",
       "      <td>73.166332</td>\n",
       "      <td>0.438122</td>\n",
       "      <td>-730.059883</td>\n",
       "      <td>-0.715745</td>\n",
       "      <td>-196.552679</td>\n",
       "      <td>-1.651703</td>\n",
       "      <td>2770.301160</td>\n",
       "      <td>0.838977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Hour  Nieuwmarkt Lat  Nieuwmarkt Lon  Nieuwezijds Kolk Lat  \\\n",
       "0 2018-03-11   100       52.371942        4.901239             52.376288   \n",
       "1 2018-03-11  2100       52.371942        4.901239             52.376288   \n",
       "2 2018-03-11  2100       52.371942        4.901239             52.376288   \n",
       "3 2018-03-11  2200       52.371942        4.901239             52.376288   \n",
       "4 2018-03-11  2200       52.371942        4.901239             52.376288   \n",
       "\n",
       "   Nieuwezijds Kolk Lon    Dam Lat  Dam Lon   Spui Lat  Spui Lon  ...  \\\n",
       "0              4.893731  52.373127  4.89008  52.369097  4.889259  ...   \n",
       "1              4.893731  52.373127  4.89008  52.369097  4.889259  ...   \n",
       "2              4.893731  52.373127  4.89008  52.369097  4.889259  ...   \n",
       "3              4.893731  52.373127  4.89008  52.369097  4.889259  ...   \n",
       "4              4.893731  52.373127  4.89008  52.369097  4.889259  ...   \n",
       "\n",
       "   Nieuwmarkt score  Nieuwmarkt weight  Nieuwezijds Kolk score  \\\n",
       "0          0.000000           0.882141                0.000000   \n",
       "1        352.242352           1.015108               83.818349   \n",
       "2        384.307874           1.107515                8.440208   \n",
       "3        342.287656           1.049962               42.314730   \n",
       "4        343.524310           1.053756               73.166332   \n",
       "\n",
       "   Nieuwezijds Kolk weight    Dam score  Dam weight  Spui score  Spui weight  \\\n",
       "0                 0.587521    -7.303229   -0.070905   -0.000000    -0.681885   \n",
       "1                 0.421198  -448.251607   -0.353790 -130.349894    -0.972760   \n",
       "2                 0.042413 -1899.118956   -1.498910 -352.192566    -2.628303   \n",
       "3                 0.253382 -1231.740804   -1.207589 -279.710090    -2.350505   \n",
       "4                 0.438122  -730.059883   -0.715745 -196.552679    -1.651703   \n",
       "\n",
       "   Centraal Station score  Centraal Station weight  \n",
       "0              163.979433                 0.346680  \n",
       "1             1668.972876                 0.432376  \n",
       "2             3994.026557                 1.034722  \n",
       "3             3446.831547                 1.043862  \n",
       "4             2770.301160                 0.838977  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DF to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(\"../../../Data_thesis/Full_Datasets/Full.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
