{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from TrainTestEvalSplit.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nbimporter\n",
    "\n",
    "import TrainTestEvalSplit as split\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#Classification Metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import check_X_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>Sensor</th>\n",
       "      <th>SensorLongitude</th>\n",
       "      <th>SensorLatitude</th>\n",
       "      <th>CrowdednessCount</th>\n",
       "      <th>Lon_4.8971927</th>\n",
       "      <th>Lon_4.8973336</th>\n",
       "      <th>...</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>Nieuwmarkt score</th>\n",
       "      <th>Nieuwezijds Kolk score</th>\n",
       "      <th>Dam score</th>\n",
       "      <th>Spui score</th>\n",
       "      <th>Centraal Station score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GAWW-04</td>\n",
       "      <td>4.897908</td>\n",
       "      <td>52.373283</td>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.996844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>472.993853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GAWW-07</td>\n",
       "      <td>4.900441</td>\n",
       "      <td>52.374414</td>\n",
       "      <td>1603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>346.998829</td>\n",
       "      <td>198.995171</td>\n",
       "      <td>1266.930956</td>\n",
       "      <td>133.989730</td>\n",
       "      <td>3859.981463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GAWW-08</td>\n",
       "      <td>4.897193</td>\n",
       "      <td>52.371650</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>346.997145</td>\n",
       "      <td>198.996668</td>\n",
       "      <td>1266.966573</td>\n",
       "      <td>133.995346</td>\n",
       "      <td>3859.909232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GAWW-09</td>\n",
       "      <td>4.898479</td>\n",
       "      <td>52.375040</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>346.997014</td>\n",
       "      <td>198.997601</td>\n",
       "      <td>1266.952991</td>\n",
       "      <td>133.991938</td>\n",
       "      <td>3859.978146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>2100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GAWW-10</td>\n",
       "      <td>4.898808</td>\n",
       "      <td>52.372369</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>346.998943</td>\n",
       "      <td>198.995907</td>\n",
       "      <td>1266.951383</td>\n",
       "      <td>133.993174</td>\n",
       "      <td>3859.941786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Hour  weekday  is_weekend   Sensor  SensorLongitude  \\\n",
       "0 2018-03-11   100      6.0         1.0  GAWW-04         4.897908   \n",
       "1 2018-03-11  2100      6.0         1.0  GAWW-07         4.900441   \n",
       "2 2018-03-11  2100      6.0         1.0  GAWW-08         4.897193   \n",
       "3 2018-03-11  2100      6.0         1.0  GAWW-09         4.898479   \n",
       "4 2018-03-11  2100      6.0         1.0  GAWW-10         4.898808   \n",
       "\n",
       "   SensorLatitude  CrowdednessCount  Lon_4.8971927  Lon_4.8973336  ...  \\\n",
       "0       52.373283               886              0              0  ...   \n",
       "1       52.374414              1603              0              0  ...   \n",
       "2       52.371650                21              1              0  ...   \n",
       "3       52.375040                88              0              0  ...   \n",
       "4       52.372369                49              0              0  ...   \n",
       "\n",
       "      month_cos   day_sin   day_cos  hour_sin  hour_cos  Nieuwmarkt score  \\\n",
       "0  6.123234e-17  0.188227  0.982126  0.258819  0.965926          0.000000   \n",
       "1  6.123234e-17  0.188227  0.982126 -0.707107  0.707107        346.998829   \n",
       "2  6.123234e-17  0.188227  0.982126 -0.707107  0.707107        346.997145   \n",
       "3  6.123234e-17  0.188227  0.982126 -0.707107  0.707107        346.997014   \n",
       "4  6.123234e-17  0.188227  0.982126 -0.707107  0.707107        346.998943   \n",
       "\n",
       "   Nieuwezijds Kolk score    Dam score  Spui score  Centraal Station score  \n",
       "0                0.000000   102.996844    0.000000              472.993853  \n",
       "1              198.995171  1266.930956  133.989730             3859.981463  \n",
       "2              198.996668  1266.966573  133.995346             3859.909232  \n",
       "3              198.997601  1266.952991  133.991938             3859.978146  \n",
       "4              198.995907  1266.951383  133.993174             3859.941786  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"../../../Data_thesis/Full_Datasets/Full.csv\")\n",
    "full_df[\"Date\"] = pd.to_datetime(full_df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0.9\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = split.clasCrowdednessCounts(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_eval, y_eval, train_dates, eval_dates = split.trainTestSplit(full_df, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest Classifier \n",
    "Implemented the [Sklearn Version](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Score:  85.64 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  84.5\n",
      "Mean Recall Score:  84.73\n",
      "Mean F1 Score:  84.59 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  82.95\n",
      "Mean Recall Score:  79.31\n",
      "Mean F1 Score:  81.08 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  86.12\n",
      "Mean Recall Score:  87.32\n",
      "Mean F1 Score:  86.71 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  88.57\n",
      "Mean Recall Score:  91.29\n",
      "Mean F1 Score:  89.89 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "mean_acc = 0\n",
    "\n",
    "mean_precision = 0\n",
    "mean_recall = 0\n",
    "mean_f1_score = 0\n",
    "\n",
    "for train_index, val_index in kf.split(train_dates):\n",
    "    \n",
    "    x_train_con, y_train_con = check_X_y(X=x_train[x_train[\"Date\"].isin(train_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                        y=y_train[y_train[\"Date\"].isin(train_dates[train_index])][\"CrowdednessCount\"])\n",
    "    rfc.fit(x_train_con,y_train_con)\n",
    "    \n",
    "    x_val_con, y_val_con = check_X_y(X=x_train[x_train[\"Date\"].isin(train_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                    y=y_train[y_train[\"Date\"].isin(train_dates[val_index])][\"CrowdednessCount\"])\n",
    "    \n",
    "    y_pred_base = rfc.predict(x_val_con)\n",
    "    \n",
    "    mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "    mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "    mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "    mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "mean_precision = (mean_precision / 10) * 100\n",
    "mean_recall = (mean_recall / 10) * 100\n",
    "mean_f1_score = (mean_f1_score / 10) * 100\n",
    "\n",
    "print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(\"For label {0}\".format(labels[i]))\n",
    "    print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "    print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "    print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  210  estimators\n",
      "Mean Accuracy Score:  79.32 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.87\n",
      "Mean Recall Score:  79.39\n",
      "Mean F1 Score:  80.3 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.05\n",
      "Mean Recall Score:  73.8\n",
      "Mean F1 Score:  74.46 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.03\n",
      "Mean Recall Score:  81.04\n",
      "Mean F1 Score:  79.09 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.1\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.0 \n",
      "\n",
      "\n",
      "\n",
      "Results for  220  estimators\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for  230  estimators\n",
      "Mean Accuracy Score:  79.3 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.87\n",
      "Mean Recall Score:  79.63\n",
      "Mean F1 Score:  80.44 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.0\n",
      "Mean Recall Score:  73.88\n",
      "Mean F1 Score:  74.51 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.07\n",
      "Mean Recall Score:  80.92\n",
      "Mean F1 Score:  79.05 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.05\n",
      "Mean Recall Score:  85.99\n",
      "Mean F1 Score:  82.83 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimators = [210, 220, 230]\n",
    "\n",
    "for n in estimators:\n",
    "    rfc = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results for \", n, \" estimators\")\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for criterion:  gini\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for criterion:  entropy\n",
      "Mean Accuracy Score:  78.67 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  80.38\n",
      "Mean Recall Score:  79.48\n",
      "Mean F1 Score:  79.64 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  75.48\n",
      "Mean Recall Score:  73.75\n",
      "Mean F1 Score:  74.2 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.11\n",
      "Mean Recall Score:  80.44\n",
      "Mean F1 Score:  78.79 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  81.63\n",
      "Mean Recall Score:  84.29\n",
      "Mean F1 Score:  81.67 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "for c in criterion:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=c, random_state=42)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results for criterion: \", c)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for feature handling:  auto\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for feature handling:  log2\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for feature handling:  None\n",
      "Mean Accuracy Score:  77.62 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  79.86\n",
      "Mean Recall Score:  78.19\n",
      "Mean F1 Score:  78.73 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  73.89\n",
      "Mean Recall Score:  72.64\n",
      "Mean F1 Score:  72.78 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  77.05\n",
      "Mean Recall Score:  80.19\n",
      "Mean F1 Score:  78.07 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  80.84\n",
      "Mean Recall Score:  82.27\n",
      "Mean F1 Score:  80.28 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\"auto\", \"log2\", None]\n",
    "\n",
    "for f in features:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=f, random_state=42)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results for feature handling: \", f)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Boostrap:  True\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for Boostrap:  False\n",
      "Mean Accuracy Score:  78.06 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  79.44\n",
      "Mean Recall Score:  79.03\n",
      "Mean F1 Score:  78.9 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  74.43\n",
      "Mean Recall Score:  72.72\n",
      "Mean F1 Score:  73.13 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  77.78\n",
      "Mean Recall Score:  80.37\n",
      "Mean F1 Score:  78.55 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  81.76\n",
      "Mean Recall Score:  83.51\n",
      "Mean F1 Score:  81.33 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bootstrap = [True, False]\n",
    "\n",
    "for b in bootstrap:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=\"auto\", bootstrap=b, random_state=42)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results for Boostrap: \", b)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOB_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Oob:  True\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results for Oob:  False\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oob = [True, False]\n",
    "\n",
    "for o in oob:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=\"auto\", bootstrap=True, random_state=42,\n",
    "                                oob_score=o)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results for Oob: \", o)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results jobs:  -1\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results jobs:  1\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n",
      "Results jobs:  10\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs = [-1, 1, 10]\n",
    "\n",
    "for n in jobs:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=\"auto\", bootstrap=True, random_state=42,\n",
    "                                oob_score=False, n_jobs=n)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results jobs: \", n)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "D:\\Programs\\Anaconda\\envs\\Analytics\\lib\\site-packages\\sklearn\\ensemble\\forest.py:308: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results warm start:  True\n",
      "Mean Accuracy Score:  97.17 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  97.07\n",
      "Mean Recall Score:  97.7\n",
      "Mean F1 Score:  97.37 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  97.16\n",
      "Mean Recall Score:  97.35\n",
      "Mean F1 Score:  97.25 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  96.15\n",
      "Mean Recall Score:  97.65\n",
      "Mean F1 Score:  96.82 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  98.32\n",
      "Mean Recall Score:  96.35\n",
      "Mean F1 Score:  97.2 \n",
      "\n",
      "\n",
      "\n",
      "Results warm start:  False\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warm = [True, False]\n",
    "\n",
    "for w in warm:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=\"auto\", bootstrap=True, random_state=42,\n",
    "                                oob_score=False, n_jobs=1, warm_start=w)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results warm start: \", w)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results weight:  balanced\n",
      "Mean Accuracy Score:  78.71 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.77\n",
      "Mean Recall Score:  79.32\n",
      "Mean F1 Score:  80.24 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  75.19\n",
      "Mean Recall Score:  73.73\n",
      "Mean F1 Score:  74.07 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  77.17\n",
      "Mean Recall Score:  80.52\n",
      "Mean F1 Score:  78.26 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  81.78\n",
      "Mean Recall Score:  84.75\n",
      "Mean F1 Score:  81.97 \n",
      "\n",
      "\n",
      "\n",
      "Results weight:  balanced_subsample\n",
      "Mean Accuracy Score:  78.75 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.04\n",
      "Mean Recall Score:  79.56\n",
      "Mean F1 Score:  79.98 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  75.56\n",
      "Mean Recall Score:  72.93\n",
      "Mean F1 Score:  73.83 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  77.47\n",
      "Mean Recall Score:  80.62\n",
      "Mean F1 Score:  78.5 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  81.97\n",
      "Mean Recall Score:  85.09\n",
      "Mean F1 Score:  82.28 \n",
      "\n",
      "\n",
      "\n",
      "Results weight:  None\n",
      "Mean Accuracy Score:  79.36 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  81.9\n",
      "Mean Recall Score:  79.8\n",
      "Mean F1 Score:  80.53 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  76.12\n",
      "Mean Recall Score:  73.62\n",
      "Mean F1 Score:  74.42 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  78.02\n",
      "Mean Recall Score:  81.0\n",
      "Mean F1 Score:  79.08 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  82.18\n",
      "Mean Recall Score:  86.2\n",
      "Mean F1 Score:  83.03 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight = [\"balanced\", \"balanced_subsample\", None]\n",
    "\n",
    "for w in weight:\n",
    "    rfc = RandomForestClassifier(n_estimators=220, criterion=\"gini\", max_features=\"auto\", bootstrap=True, random_state=42,\n",
    "                                oob_score=False, n_jobs=1, warm_start=False, class_weight=w)\n",
    "    \n",
    "    labels = [1, 2, 3, 4]\n",
    "\n",
    "    mean_acc = 0\n",
    "\n",
    "    mean_precision = 0\n",
    "    mean_recall = 0\n",
    "    mean_f1_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(eval_dates):\n",
    "\n",
    "        x_train_con, y_train_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[train_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[train_index])][\"CrowdednessCount\"])\n",
    "        rfc.fit(x_train_con,y_train_con)\n",
    "\n",
    "        x_val_con, y_val_con = check_X_y(X=x_eval[x_eval[\"Date\"].isin(eval_dates[val_index])].drop(columns={\"Date\"}),\n",
    "                                            y=y_eval[y_eval[\"Date\"].isin(eval_dates[val_index])][\"CrowdednessCount\"])\n",
    "\n",
    "        y_pred_base = rfc.predict(x_val_con)\n",
    "\n",
    "        mean_acc += accuracy_score(y_val_con, y_pred_base)\n",
    "\n",
    "        mean_precision += precision_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_recall += recall_score(y_val_con, y_pred_base, average=None)\n",
    "        mean_f1_score += f1_score(y_val_con, y_pred_base, average=None)\n",
    "\n",
    "    mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "    mean_precision = (mean_precision / 10) * 100\n",
    "    mean_recall = (mean_recall / 10) * 100\n",
    "    mean_f1_score = (mean_f1_score / 10) * 100\n",
    "    \n",
    "    print(\"Results weight: \", w)\n",
    "\n",
    "    print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        print(\"For label {0}\".format(labels[i]))\n",
    "        print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "        print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "        print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost \n",
    "Implement [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.XGBClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Score:  81.24 \n",
      "\n",
      "For label 1\n",
      "Mean Precision Score:  85.74\n",
      "Mean Recall Score:  79.09\n",
      "Mean F1 Score:  82.24 \n",
      "\n",
      "For label 2\n",
      "Mean Precision Score:  77.31\n",
      "Mean Recall Score:  75.55\n",
      "Mean F1 Score:  76.41 \n",
      "\n",
      "For label 3\n",
      "Mean Precision Score:  81.23\n",
      "Mean Recall Score:  77.62\n",
      "Mean F1 Score:  79.36 \n",
      "\n",
      "For label 4\n",
      "Mean Precision Score:  80.92\n",
      "Mean Recall Score:  93.09\n",
      "Mean F1 Score:  86.49 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [1, 2, 3, 4]\n",
    "\n",
    "mean_acc = 0\n",
    "\n",
    "mean_precision = 0\n",
    "mean_recall = 0\n",
    "mean_f1_score = 0\n",
    "\n",
    "for train_index, val_index in kf.split(train_dates):\n",
    "    \n",
    "    xgb.fit(x_train[x_train[\"Date\"].isin(train_dates[train_index])].drop(columns={\"Date\"}),\n",
    "            y_train[y_train[\"Date\"].isin(train_dates[train_index])][\"CrowdednessCount\"])\n",
    "    \n",
    "    y_pred_base = xgb.predict(x_train[x_train[\"Date\"].isin(train_dates[val_index])].drop(columns={\"Date\"}))\n",
    "    \n",
    "    mean_acc += accuracy_score(y_train[y_train[\"Date\"].isin(train_dates[val_index])][\"CrowdednessCount\"], y_pred_base)\n",
    "\n",
    "    mean_precision += precision_score(y_train[y_train[\"Date\"].isin(train_dates[val_index])][\"CrowdednessCount\"], y_pred_base, average=None)\n",
    "    mean_recall += recall_score(y_train[y_train[\"Date\"].isin(train_dates[val_index])][\"CrowdednessCount\"], y_pred_base, average=None)\n",
    "    mean_f1_score += f1_score(y_train[y_train[\"Date\"].isin(train_dates[val_index])][\"CrowdednessCount\"], y_pred_base, average=None)\n",
    "\n",
    "mean_acc = round(((mean_acc / 10) * 100),2)\n",
    "mean_precision = (mean_precision / 10) * 100\n",
    "mean_recall = (mean_recall / 10) * 100\n",
    "mean_f1_score = (mean_f1_score / 10) * 100\n",
    "\n",
    "print(\"Mean Accuracy Score: \", mean_acc, \"\\n\")\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(\"For label {0}\".format(labels[i]))\n",
    "    print(\"Mean Precision Score: \", round(mean_precision[i], 2))\n",
    "    print(\"Mean Recall Score: \", round(mean_recall[i], 2))\n",
    "    print(\"Mean F1 Score: \", round(mean_f1_score[i], 2), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
